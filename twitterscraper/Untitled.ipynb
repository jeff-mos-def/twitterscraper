{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement billiard.pool (from versions: none)\n",
      "ERROR: No matching distribution found for billiard.pool\n"
     ]
    }
   ],
   "source": [
    "pip install billiard.pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'billiard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f8a383f5b778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbilliard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'billiard'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import urllib\n",
    "import random\n",
    "import datetime as dt\n",
    "\n",
    "from functools import partial\n",
    "from billiard.pool import Pool\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import cycle\n",
    "\n",
    "from twitterscraper.tweet import Tweet\n",
    "from twitterscraper.ts_logger import logger\n",
    "from twitterscraper.user import User\n",
    "\n",
    "#from fake_useragent import UserAgent\n",
    "#ua = UserAgent()\n",
    "#HEADER = {'User-Agent': ua.random}\n",
    "HEADERS_LIST = [\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; x64; fr; rv:1.9.2.13) Gecko/20101203 Firebird/3.6.13',\n",
    "    'Mozilla/5.0 (compatible, MSIE 11, Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',\n",
    "    'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
    "    'Mozilla/5.0 (Windows NT 5.2; RW; rv:7.0a1) Gecko/20091211 SeaMonkey/9.23a1pre'\n",
    "]\n",
    "\n",
    "HEADER = {'User-Agent': random.choice(HEADERS_LIST)}\n",
    "logger.info(HEADER)\n",
    "\n",
    "INIT_URL = 'https://twitter.com/search?f=tweets&vertical=default&q={q}&l={lang}'\n",
    "RELOAD_URL = 'https://twitter.com/i/search/timeline?f=tweets&vertical=' \\\n",
    "             'default&include_available_features=1&include_entities=1&' \\\n",
    "             'reset_error_state=false&src=typd&max_position={pos}&q={q}&l={lang}'\n",
    "INIT_URL_USER = 'https://twitter.com/{u}'\n",
    "RELOAD_URL_USER = 'https://twitter.com/i/profiles/show/{u}/timeline/tweets?' \\\n",
    "                  'include_available_features=1&include_entities=1&' \\\n",
    "                  'max_position={pos}&reset_error_state=false'\n",
    "PROXY_URL = 'https://free-proxy-list.net/'\n",
    "\n",
    "def get_proxies():\n",
    "    response = requests.get(PROXY_URL)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    table = soup.find('table',id='proxylisttable')\n",
    "    list_tr = table.find_all('tr')\n",
    "    list_td = [elem.find_all('td') for elem in list_tr]\n",
    "    list_td = list(filter(None, list_td))\n",
    "    list_ip = [elem[0].text for elem in list_td]\n",
    "    list_ports = [elem[1].text for elem in list_td]\n",
    "    list_proxies = [':'.join(elem) for elem in list(zip(list_ip, list_ports))]\n",
    "    return list_proxies               \n",
    "                  \n",
    "def get_query_url(query, lang, pos, from_user = False):\n",
    "    if from_user:\n",
    "        if pos is None:\n",
    "            return INIT_URL_USER.format(u=query)\n",
    "        else:\n",
    "            return RELOAD_URL_USER.format(u=query, pos=pos)\n",
    "    if pos is None:\n",
    "        return INIT_URL.format(q=query, lang=lang)\n",
    "    else:\n",
    "        return RELOAD_URL.format(q=query, pos=pos, lang=lang)\n",
    "\n",
    "def linspace(start, stop, n):\n",
    "    if n == 1:\n",
    "        yield stop\n",
    "        return\n",
    "    h = (stop - start) / (n - 1)\n",
    "    for i in range(n):\n",
    "        yield start + h * i\n",
    "\n",
    "proxies = get_proxies()\n",
    "proxy_pool = cycle(proxies)\n",
    "\n",
    "def query_single_page(query, lang, pos, retry=50, from_user=False, timeout=60):\n",
    "    \"\"\"\n",
    "    Returns tweets from the given URL.\n",
    "\n",
    "    :param query: The query parameter of the query url\n",
    "    :param lang: The language parameter of the query url\n",
    "    :param pos: The query url parameter that determines where to start looking\n",
    "    :param retry: Number of retries if something goes wrong.\n",
    "    :return: The list of tweets, the pos argument for getting the next page.\n",
    "    \"\"\"\n",
    "    url = get_query_url(query, lang, pos, from_user)\n",
    "    logger.info('Scraping tweets from {}'.format(url))\n",
    "\n",
    "    try:\n",
    "        proxy = next(proxy_pool)\n",
    "        logger.info('Using proxy {}'.format(proxy))\n",
    "        response = requests.get(url, headers=HEADER, proxies={\"http\": proxy}, timeout=timeout)\n",
    "        if pos is None:  # html response\n",
    "            html = response.text or ''\n",
    "            json_resp = None\n",
    "        else:\n",
    "            html = ''\n",
    "            try:\n",
    "                json_resp = response.json()\n",
    "                html = json_resp['items_html'] or ''\n",
    "            except ValueError as e:\n",
    "                logger.exception('Failed to parse JSON \"{}\" while requesting \"{}\"'.format(e, url))\n",
    "\n",
    "        tweets = list(Tweet.from_html(html))\n",
    "\n",
    "        if not tweets:\n",
    "            try:\n",
    "                if json_resp:\n",
    "                    pos = json_resp['min_position']\n",
    "                    has_more_items = json_resp['has_more_items']\n",
    "                    if not has_more_items:\n",
    "                        logger.info(\"Twitter returned : 'has_more_items' \")\n",
    "                        return [], None\n",
    "                else:\n",
    "                    pos = None\n",
    "            except:\n",
    "                pass\n",
    "            if retry > 0:\n",
    "                logger.info('Retrying... (Attempts left: {})'.format(retry))\n",
    "                return query_single_page(query, lang, pos, retry - 1, from_user)\n",
    "            else:\n",
    "                return [], pos\n",
    "\n",
    "        if json_resp:\n",
    "            return tweets, urllib.parse.quote(json_resp['min_position'])\n",
    "        if from_user:\n",
    "            return tweets, tweets[-1].tweet_id\n",
    "        return tweets, \"TWEET-{}-{}\".format(tweets[-1].tweet_id, tweets[0].tweet_id)\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.exception('HTTPError {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        logger.exception('ConnectionError {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        logger.exception('TimeOut {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    except json.decoder.JSONDecodeError as e:\n",
    "        logger.exception('Failed to parse JSON \"{}\" while requesting \"{}\".'.format(\n",
    "            e, url))\n",
    "\n",
    "    if retry > 0:\n",
    "        logger.info('Retrying... (Attempts left: {})'.format(retry))\n",
    "        return query_single_page(query, lang, pos, retry - 1)\n",
    "\n",
    "    logger.error('Giving up.')\n",
    "    return [], None\n",
    "\n",
    "\n",
    "def query_tweets_once_generator(query, limit=None, lang='', pos=None):\n",
    "    \"\"\"\n",
    "    Queries twitter for all the tweets you want! It will load all pages it gets\n",
    "    from twitter. However, twitter might out of a sudden stop serving new pages,\n",
    "    in that case, use the `query_tweets` method.\n",
    "\n",
    "    Note that this function catches the KeyboardInterrupt so it can return\n",
    "    tweets on incomplete queries if the user decides to abort.\n",
    "\n",
    "    :param query: Any advanced query you want to do! Compile it at\n",
    "                  https://twitter.com/search-advanced and just copy the query!\n",
    "    :param limit: Scraping will be stopped when at least ``limit`` number of\n",
    "                  items are fetched.\n",
    "    :param pos: Field used as a \"checkpoint\" to continue where you left off in iteration\n",
    "    :return:      A list of twitterscraper.Tweet objects. You will get at least\n",
    "                  ``limit`` number of items.\n",
    "    \"\"\"\n",
    "    logger.info('Querying {}'.format(query))\n",
    "    query = query.replace(' ', '%20').replace('#', '%23').replace(':', '%3A').replace('&', '%26')\n",
    "    num_tweets = 0\n",
    "    try:\n",
    "        while True:\n",
    "            new_tweets, new_pos = query_single_page(query, lang, pos)\n",
    "            if len(new_tweets) == 0:\n",
    "                logger.info('Got {} tweets for {}.'.format(\n",
    "                    num_tweets, query))\n",
    "                return\n",
    "\n",
    "            for t in new_tweets:\n",
    "                yield t, pos\n",
    "\n",
    "            # use new_pos only once you have iterated through all old tweets\n",
    "            pos = new_pos\n",
    "\n",
    "            num_tweets += len(new_tweets)\n",
    "\n",
    "            if limit and num_tweets >= limit:\n",
    "                logger.info('Got {} tweets for {}.'.format(\n",
    "                    num_tweets, query))\n",
    "                return\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info('Program interrupted by user. Returning tweets gathered '\n",
    "                     'so far...')\n",
    "    except BaseException:\n",
    "        logger.exception('An unknown error occurred! Returning tweets '\n",
    "                          'gathered so far.')\n",
    "    logger.info('Got {} tweets for {}.'.format(\n",
    "        num_tweets, query))\n",
    "\n",
    "\n",
    "def query_tweets_once(*args, **kwargs):\n",
    "    res = list(query_tweets_once_generator(*args, **kwargs))\n",
    "    if res:\n",
    "        tweets, positions = zip(*res)\n",
    "        return tweets\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def query_tweets(query, limit=None, begindate=dt.date(2006, 3, 21), enddate=dt.date.today(), poolsize=20, lang=''):\n",
    "    no_days = (enddate - begindate).days\n",
    "    \n",
    "    if(no_days < 0):\n",
    "        sys.exit('Begin date must occur before end date.')\n",
    "    \n",
    "    if poolsize > no_days:\n",
    "        # Since we are assigning each pool a range of dates to query,\n",
    "\t\t# the number of pools should not exceed the number of dates.\n",
    "        poolsize = no_days\n",
    "    dateranges = [begindate + dt.timedelta(days=elem) for elem in linspace(0, no_days, poolsize+1)]\n",
    "\n",
    "    if limit and poolsize:\n",
    "        limit_per_pool = (limit // poolsize)+1\n",
    "    else:\n",
    "        limit_per_pool = None\n",
    "\n",
    "    queries = ['{} since:{} until:{}'.format(query, since, until)\n",
    "               for since, until in zip(dateranges[:-1], dateranges[1:])]\n",
    "\n",
    "    all_tweets = []\n",
    "    try:\n",
    "        pool = Pool(poolsize)\n",
    "        logger.info('queries: {}'.format(queries))\n",
    "        try:\n",
    "            for new_tweets in pool.imap_unordered(partial(query_tweets_once, limit=limit_per_pool, lang=lang), queries):\n",
    "                all_tweets.extend(new_tweets)\n",
    "                logger.info('Got {} tweets ({} new).'.format(\n",
    "                    len(all_tweets), len(new_tweets)))\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info('Program interrupted by user. Returning all tweets '\n",
    "                         'gathered so far.')\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    return all_tweets\n",
    "\n",
    "\n",
    "def query_tweets_from_user(user, limit=None):\n",
    "    pos = None\n",
    "    tweets = []\n",
    "    try:\n",
    "        while True:\n",
    "           new_tweets, pos = query_single_page(user, lang='', pos=pos, from_user=True)\n",
    "           if len(new_tweets) == 0:\n",
    "               logger.info(\"Got {} tweets from username {}\".format(len(tweets), user))\n",
    "               return tweets\n",
    "\n",
    "           tweets += new_tweets\n",
    "\n",
    "           if limit and len(tweets) >= limit:\n",
    "               logger.info(\"Got {} tweets from username {}\".format(len(tweets), user))\n",
    "               return tweets\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Program interrupted by user. Returning tweets gathered \"\n",
    "                     \"so far...\")\n",
    "    except BaseException:\n",
    "        logger.exception(\"An unknown error occurred! Returning tweets \"\n",
    "                          \"gathered so far.\")\n",
    "    logger.info(\"Got {} tweets from username {}.\".format(\n",
    "        len(tweets), user))\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def query_user_page(url, retry=10, timeout=60):\n",
    "    \"\"\"\n",
    "    Returns the scraped user data from a twitter user page.\n",
    "\n",
    "    :param url: The URL to get the twitter user info from (url contains the user page)\n",
    "    :param retry: Number of retries if something goes wrong.\n",
    "    :return: Returns the scraped user data from a twitter user page.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        proxy = next(proxy_pool)\n",
    "        logger.info('Using proxy {}'.format(proxy))\n",
    "        response = requests.get(url, headers=HEADER, proxies={\"http\": proxy})\n",
    "        html = response.text or ''\n",
    "\n",
    "        user_info = User.from_html(html)\n",
    "        if not user_info:\n",
    "            return None\n",
    "\n",
    "        return user_info\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.exception('HTTPError {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        logger.exception('ConnectionError {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        logger.exception('TimeOut {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "\n",
    "    if retry > 0:\n",
    "        logger.info('Retrying... (Attempts left: {})'.format(retry))\n",
    "        return query_user_page(url, retry-1)\n",
    "\n",
    "    logger.error('Giving up.')\n",
    "    return None\n",
    "\n",
    "\n",
    "def query_user_info(user):\n",
    "    \"\"\"\n",
    "    Returns the scraped user data from a twitter user page.\n",
    "\n",
    "    :param user: the twitter user to web scrape its twitter page info\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        user_info = query_user_page(INIT_URL_USER.format(u=user))\n",
    "        if user_info:\n",
    "            logger.info(\"Got user information from username {}\".format(user))\n",
    "            return user_info\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Program interrupted by user. Returning user information gathered so far...\")\n",
    "    except BaseException:\n",
    "        logger.exception(\"An unknown error occurred! Returning user information gathered so far...\")\n",
    "\n",
    "    logger.info(\"Got user information from username {}\".format(user))\n",
    "    return user_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['notre dame fire since:2019-04-15 until:2019-04-16', 'notre dame fire since:2019-04-16 until:2019-04-17', 'notre dame fire since:2019-04-17 until:2019-04-18']\n",
      "INFO: Got 348 tweets (348 new).\n",
      "INFO: Got 698 tweets (350 new).\n",
      "INFO: Got 1039 tweets (341 new).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tweet' object has no attribute '_dict_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-411e0b0dcd8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'notre dame fire'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegindate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbegin_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menddate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dict_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExtensionArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ndim\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-411e0b0dcd8e>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'notre dame fire'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegindate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbegin_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menddate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dict_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tweet' object has no attribute '_dict_'"
     ]
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "import datetime as dt \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "begin_date = dt.date(2019,4,15)\n",
    "end_date = dt.date(2019,4,18)\n",
    "\n",
    "\n",
    "limit = 1000\n",
    "lang = 'english'\n",
    "\n",
    "\n",
    "tweets = query_tweets('notre dame fire', begindate = begin_date, enddate = end_date, limit = limit, lang = lang)\n",
    "df = pd.DataFrame(t._dict_ for t in tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
